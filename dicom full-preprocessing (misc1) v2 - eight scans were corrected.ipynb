{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom # module for reading dicom files\n",
    "import os # module for interfacing with the os\n",
    "import numpy as np # numpy for arrays etc\n",
    "import pandas as pd # module for creating and querying data tables (databases) efficiently\n",
    "import pylidc as pl # module for handling the LIDC dataset\n",
    "import matplotlib.pyplot as plt # plotting utilities\n",
    "import pdb # debugger\n",
    "import scipy.ndimage # \n",
    "import scipy.sparse\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy\n",
    "from utils.preprocess_functions import *\n",
    "from pylidc.utils import consensus\n",
    "from skimage import measure, morphology\n",
    "from skimage.morphology import ball, dilation, disk, erosion\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to the previous version (v2), this script removes the scans with bad\n",
    "# slices (>2.5mm or inconsistency between spacing and thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the relevant paths\n",
    "\n",
    "LIDC_PATH = '/data/datasets/LIDC-IDRI/' # original LIDC data\n",
    "# annotations = pd.read_csv('/data/datasets/LIDC-IDRI/annotations.csv')\n",
    "LIDC_IDs = os.listdir(f'{LIDC_PATH}LIDC-IDRI')\n",
    "LIDC_IDs = [i for i in LIDC_IDs if 'LIDC' in i]\n",
    "LIDC_IDs = np.sort(LIDC_IDs)\n",
    "\n",
    "# output path\n",
    "path_dest = f'/data/OMM/Datasets/LIDC_other_formats/LIDC_preprocessed_3D v2/'\n",
    "if not os.path.exists(path_dest): os.makedirs(path_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all the scans for X patient(s)\n",
    "df = pd.read_csv('/data/datasets/LIDC-IDRI/annotations.csv')\n",
    "\n",
    "#%%\n",
    "scans_with_errors = []\n",
    "errorScansFile = open(path_dest + \"scans_with_errors.txt\",\"w\")\n",
    "\n",
    "numVoxelsPerLungSeg = []\n",
    "listOfRejectedPatients = []\n",
    "rejectListFile = open(path_dest + \"rejectedPatients.txt\",\"w\") \n",
    "\n",
    "listOfUsedPatients = []\n",
    "useListFile = open(path_dest + \"usedPatients.txt\",\"w\") \n",
    "\n",
    "requiredSelemWidth = []\n",
    "selemZWidthFile = open(path_dest + \"segmentationSelemZWidths.txt\",\"w\") \n",
    "\n",
    "for idx, k in enumerate(LIDC_IDs):\n",
    "\n",
    "#     if idx <927:continue\n",
    "    \n",
    "#    #idx = 11\n",
    "#    #idx = 611\n",
    "#    idx = 309\n",
    "#    k = LIDC_IDs[idx]\n",
    "\n",
    "    #if idx>5:break\n",
    "    print(f'preprocessing: {idx}, {k}')\n",
    "       \n",
    "    df_patient = df.loc[df['patientid']==int(k[-4:])] \n",
    "    pid = k\n",
    "    \n",
    "    # query the LIDC images with patient_id = pid \n",
    "    # HERE WE JUST USE THE FIRST ONE!!\n",
    "    idx_scan = 0 \n",
    "    \n",
    "    # get the scan object for this scan\n",
    "    scan = pl.query(pl.Scan).filter(pl.Scan.patient_id == pid)[idx_scan] \n",
    "    \n",
    "    # here we can reject according to any criteria we like\n",
    "    thickSlice = (scan.slice_thickness > 3) | (scan.slice_spacing > 3)\n",
    "    missingSlices = len(np.unique(np.round(100*np.diff(scan.slice_zvals)))) != 1\n",
    "    if (thickSlice)  :\n",
    "        # we want to reject this scan/patient\n",
    "        print('Undesirable slice characteristics, rejecting')\n",
    "        listOfRejectedPatients.append(pid)\n",
    "        continue\n",
    "    elif (missingSlices):\n",
    "        print('#################################')\n",
    "        listOfRejectedPatients.append(pid)\n",
    "        continue\n",
    "    else:\n",
    "        # we will use this scan\n",
    "        listOfUsedPatients.append(pid)\n",
    "        #continue # this lets us quickly check the outcome of the selection\n",
    "    \n",
    "    print('Loading and converting to HU')\n",
    "    curr_patient_pixels, spacing_orig = custom_load_scan_to_HU(scan)\n",
    "\n",
    "    print('Resampling to isotropic resolution')\n",
    "    pix_resampled, spacing = resample_grid(curr_patient_pixels, spacing_orig, [1,1,1])\n",
    "    \n",
    "    print('Segmenting the lungs and dilating the mask')\n",
    "    segmented_lungs_fill, requiredSelemWidthTmp = segment_lung_mask_SE(pix_resampled, True)\n",
    "    requiredSelemWidth.append(requiredSelemWidthTmp)\n",
    "    # Dilate the mask\n",
    "    selem = ball(5) # radius of 5 mm\n",
    "    dilated = dilation(segmented_lungs_fill, selem) # dilate a bit according to the tut\n",
    "    # Apply the mask\n",
    "    pix_resampled_to_use = pix_resampled*dilated\n",
    "    # count the number of lung voxels to find those which are badly segmented\n",
    "    numVoxelsPerLungSeg.append(np.count_nonzero(dilated))\n",
    "    \n",
    "    print('Finding nodule masks')\n",
    "    # The mask\n",
    "    # put the mask on an array with the same shape as the original volume\n",
    "    one_segmentation_consensus = np.zeros_like(curr_patient_pixels)\n",
    "    one_segmentation_maxvol = np.zeros_like(curr_patient_pixels)\n",
    "     \n",
    "    # get all the annotations for this scan\n",
    "    ids = [i.id for i in scan.annotations] # this gives the annotation IDs (note that they are not in order in the annotations.csv)\n",
    "     \n",
    "    # we split the df for patient pid into the part for just this scan\n",
    "    df_patient_partX = df_patient.loc[df_patient.annotation_id.isin(ids)]\n",
    "    unique_nodules = np.unique(df_patient_partX['cluster_id'].values)\n",
    "    nods = scan.cluster_annotations() # get the annotations for all nodules in this scan\n",
    "\n",
    "    for unique_nodule in unique_nodules:\n",
    "        df_nodule = df_patient_partX.loc[df_patient_partX['cluster_id']==unique_nodule] # this gives all annotations for this nodule (cluster)\n",
    "     \n",
    "        anns = nods[unique_nodule] # then choose the annotation for the nodule we are considering\n",
    "    \n",
    "        try:\n",
    "            # cmask = consensus mask, cbbox = consensus bounding box, masks = original annotations\n",
    "            cmask,cbbox,masks = consensus(anns, clevel=0.5, pad=[(0,0), (0,0), (0,0)])\n",
    "        except NameError:\n",
    "            scans_with_errors.append(pid)\n",
    "            continue\n",
    "     \n",
    "        # we want to save the consensus AND the mask of all segmented voxels in all annotations\n",
    "        one_mask_consensus = cmask\n",
    "        one_mask_maxvol = np.zeros_like(cmask)\n",
    "        for mask in masks:\n",
    "            one_mask_maxvol = (one_mask_maxvol > 0) | (mask > 0)    \n",
    "        \n",
    "        # pylidc loads in a different order to our custom 3D dicom reader, so need to swap dims\n",
    "        one_mask_consensus = np.swapaxes(one_mask_consensus,1,2);one_mask_consensus = np.swapaxes(one_mask_consensus,0,1)\n",
    "        one_mask_maxvol = np.swapaxes(one_mask_maxvol,1,2);one_mask_maxvol = np.swapaxes(one_mask_maxvol,0,1)\n",
    "        \n",
    "        # fill the consensus bounding box with the mask to get a nodule segmentation in original image space (presumably the cbbox is big enough for all the individual masks)\n",
    "        one_segmentation_consensus[cbbox[2].start:cbbox[2].stop,cbbox[0].start:cbbox[0].stop,cbbox[1].start:cbbox[1].stop] = one_mask_consensus\n",
    "        one_segmentation_maxvol[cbbox[2].start:cbbox[2].stop,cbbox[0].start:cbbox[0].stop,cbbox[1].start:cbbox[1].stop] = one_mask_maxvol\n",
    "\n",
    "    pass \n",
    "\n",
    "    # Resample the nodule segmentation\n",
    "    mask_consensus_resampled, _ = resample_grid(one_segmentation_consensus, spacing_orig, [1,1,1]) # first patient still has the voxel size of the original image to enable the resampling\n",
    "    mask_maxvol_resampled, _ = resample_grid(one_segmentation_maxvol, spacing_orig, [1,1,1]) # first patient still has the voxel size of the original image to enable the resampling\n",
    "\n",
    "    print('Saving...')\n",
    "    # now we save the results, saving each slice as a sparse array to cut down on size!\n",
    "    # (currently just saving the last nodule per scan?)\n",
    "    if not os.path.exists(f'{path_dest}{k}/scans'): os.makedirs(f'{path_dest}{k}/scans')\n",
    "    if not os.path.exists(f'{path_dest}{k}/consensus_masks'): os.makedirs(f'{path_dest}{k}/consensus_masks')\n",
    "    if not os.path.exists(f'{path_dest}{k}/maxvol_masks'): os.makedirs(f'{path_dest}{k}/maxvol_masks')\n",
    "    if not os.path.exists(f'{path_dest}{k}/lung_masks'): os.makedirs(f'{path_dest}{k}/lung_masks')\n",
    "\n",
    "    for idj,(slice_pix, slice_mask_consensus, slice_mask_maxvol,slice_lungseg) in enumerate(zip(pix_resampled_to_use, mask_consensus_resampled, mask_maxvol_resampled,dilated)):\n",
    "        sparse_matrix = scipy.sparse.csc_matrix(slice_pix)\n",
    "        sparse_matrix2 = scipy.sparse.csc_matrix(slice_mask_consensus)\n",
    "        sparse_matrix3 = scipy.sparse.csc_matrix(slice_mask_maxvol)\n",
    "        sparse_matrix4 = scipy.sparse.csc_matrix(slice_lungseg)\n",
    "        \n",
    "        scipy.sparse.save_npz(f'{path_dest}{k}/scans/slice_{idj:04d}.npz', sparse_matrix, compressed=True)\n",
    "        scipy.sparse.save_npz(f'{path_dest}{k}/consensus_masks/slice_m_{idj:04d}.npz', sparse_matrix2, compressed=True)\n",
    "        scipy.sparse.save_npz(f'{path_dest}{k}/maxvol_masks/slice_m_{idj:04d}.npz', sparse_matrix3, compressed=True)\n",
    "        scipy.sparse.save_npz(f'{path_dest}{k}/lung_masks/slice_m_{idj:04d}.npz', sparse_matrix4, compressed=True)\n",
    "\n",
    "#%% save some summary output\n",
    "np.savetxt('segmentation_results.dat', numVoxelsPerLungSeg)\n",
    "\n",
    "np.savetxt(rejectListFile,listOfRejectedPatients,'%10s')\n",
    "rejectListFile.close()\n",
    "\n",
    "np.savetxt(useListFile,listOfUsedPatients,'%10s')\n",
    "useListFile.close()\n",
    "\n",
    "np.savetxt(selemZWidthFile,requiredSelemWidth,'%u')\n",
    "selemZWidthFile.close()\n",
    "\n",
    "np.savetxt(errorScansFile,scans_with_errors,'%10s')\n",
    "errorScansFile.close()\n",
    "\n",
    "#%% plot segmentation results\n",
    "ax = plt.hist(numVoxelsPerLungSeg,100)\n",
    "plt.xlabel('Number of voxels in segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
